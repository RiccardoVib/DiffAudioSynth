# Copyright (C) 2025 Riccardo Simionato, University of Oslo
# Inquiries: riccardo.simionato.vib@gmail.com.com
#
# This code is free software: you can redistribute it and/or modify it under the terms
# of the GNU Lesser General Public License as published by the Free Software Foundation,
# either version 3 of the License, or (at your option) any later version.
#
# This code is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
# See the GNU Less General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License along with this code.
# If not, see <http://www.gnu.org/licenses/>.

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import List, Optional, Union, Callable
from einops import rearrange

class FiLM(nn.Module):
    """
    Feature-wise Linear Modulation Layer.

    FiLM applies an affine transformation to features conditioned on some input.
    It's commonly used in conditional batch normalization and for modulating
    neural networks based on external conditioning information.

    Formula: FiLM(x) = gamma * x + beta
    where gamma and beta are generated by a conditioning network.
    """

    def __init__(
            self,
            input_dim: int,
            condition_dim: int,
            activation: Optional[Union[str, Callable]] = "relu",
            use_layer_norm: bool = False,
    ):
        """
        Initialize a FiLM layer.

        Args:
            input_dim: Dimension of features to be modulated
            condition_dim: Dimension of conditioning input
            activation: Activation function to use in the conditioning network
            use_layer_norm: Whether to use layer normalization before modulation
        """
        super(FiLM, self).__init__()

        self.input_dim = input_dim
        self.condition_dim = condition_dim
        self.use_layer_norm = use_layer_norm

        # Activation function
        if activation is None:
            self.activation_fn = lambda x: x
        elif isinstance(activation, str):
            activation_funcs = {
                "relu": F.relu,
                "leaky_relu": F.leaky_relu,
                "tanh": torch.tanh,
                "sigmoid": torch.sigmoid,
                "elu": F.elu,
                "gelu": F.gelu
            }
            self.activation_fn = activation_funcs.get(activation.lower(), F.relu)
        else:
            self.activation_fn = activation

        # Simple linear mapping from condition to modulation parameters
        self.conditioning_net = nn.Linear(condition_dim, 2 * input_dim)

        # Layer normalization
        if use_layer_norm:
            self.layer_norm = nn.LayerNorm(input_dim)


    def forward(self, x: torch.Tensor, condition: torch.Tensor) -> torch.Tensor:
        """
        Apply FiLM modulation to input features based on condition.

        Args:
            x: Input features to modulate [batch_size, ..., input_dim]
            condition: Conditioning input [batch_size, ..., condition_dim]

        Returns:
            Modulated features with the same shape as x
        """
        # Apply layer normalization if enabled
        if self.use_layer_norm:
            x = self.layer_norm(x)

        # Generate gamma and beta from condition
        film_params = self.conditioning_net(condition)

        # Split into gamma and beta
        gamma, beta = torch.split(film_params, self.input_dim, dim=-1)


        # Apply modulation: gamma * x + beta
        return gamma * x + beta


