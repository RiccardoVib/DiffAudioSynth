import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import List, Optional, Union, Callable
from einops import rearrange

class FiLM(nn.Module):
    """
    Feature-wise Linear Modulation Layer.

    FiLM applies an affine transformation to features conditioned on some input.
    It's commonly used in conditional batch normalization and for modulating
    neural networks based on external conditioning information.

    Formula: FiLM(x) = gamma * x + beta
    where gamma and beta are generated by a conditioning network.
    """

    def __init__(
            self,
            input_dim: int,
            condition_dim: int,
            activation: Optional[Union[str, Callable]] = "relu",
            use_layer_norm: bool = False,
    ):
        """
        Initialize a FiLM layer.

        Args:
            input_dim: Dimension of features to be modulated
            condition_dim: Dimension of conditioning input
            activation: Activation function to use in the conditioning network
            use_layer_norm: Whether to use layer normalization before modulation
        """
        super(FiLM, self).__init__()

        self.input_dim = input_dim
        self.condition_dim = condition_dim
        self.use_layer_norm = use_layer_norm

        # Activation function
        if activation is None:
            self.activation_fn = lambda x: x
        elif isinstance(activation, str):
            activation_funcs = {
                "relu": F.relu,
                "leaky_relu": F.leaky_relu,
                "tanh": torch.tanh,
                "sigmoid": torch.sigmoid,
                "elu": F.elu,
                "gelu": F.gelu
            }
            self.activation_fn = activation_funcs.get(activation.lower(), F.relu)
        else:
            self.activation_fn = activation

        # Simple linear mapping from condition to modulation parameters
        self.conditioning_net = nn.Linear(condition_dim, 2 * input_dim)

        # Layer normalization
        if use_layer_norm:
            self.layer_norm = nn.LayerNorm(input_dim)


    def forward(self, x: torch.Tensor, condition: torch.Tensor) -> torch.Tensor:
        """
        Apply FiLM modulation to input features based on condition.

        Args:
            x: Input features to modulate [batch_size, ..., input_dim]
            condition: Conditioning input [batch_size, ..., condition_dim]

        Returns:
            Modulated features with the same shape as x
        """
        # Apply layer normalization if enabled
        if self.use_layer_norm:
            x = self.layer_norm(x)

        # Generate gamma and beta from condition
        film_params = self.conditioning_net(condition)

        # Split into gamma and beta
        gamma, beta = torch.split(film_params, self.input_dim, dim=-1)


        # Apply modulation: gamma * x + beta
        return gamma * x + beta


if __name__ == "__main__":
    batch_size = 16
    frame_size = 256
    input_shape = (batch_size, frame_size, 1)
    input = torch.randn(input_shape)
    num_contioning = 1
    continioning = torch.randn((batch_size, frame_size, num_contioning))
    print(f"input: {input}")
    film = FiLM(input_dim=1, condition_dim=num_contioning, use_layer_norm=True)
    output = film(input, continioning)
    print(f"output: {output}")

